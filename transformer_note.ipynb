{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer网络结构解读及Pytorch简单实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1、前言\n",
    "transformer作为大模型基础的网络架构，了解其中的实现原理是AI从业人员的基础要求。本文借助网络资料及Chat-gpt，基于Pytorch实现了一个基本的Transform模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"700\"\n",
       "            src=\"https://arxiv.org/pdf/1706.03762\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa5cce12e90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1706.03762', width=1000, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2、词向量及位置编码\n",
    "一般而言，Transformer的介绍基本从Attention机制说起，但是本文的思路是构造一个端到端的Transformer模型用于训练，所以我们先从模型的输入开始。\n",
    "\n",
    "### 2.1、词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq: tensor([[1, 2, 3, 4, 5]])\n",
      "embedded_seq:\n",
      " tensor([[[ 0.6954,  0.9693,  1.3929,  ...,  0.8606, -1.6242,  0.5194],\n",
      "         [-0.4321,  1.1273,  0.4393,  ...,  0.4213,  2.0484,  1.2938],\n",
      "         [-0.7763, -0.2724,  0.1364,  ..., -0.1198, -0.8255, -0.2942],\n",
      "         [-0.2569,  0.9213,  0.0094,  ...,  0.6287, -0.4491, -1.3598],\n",
      "         [ 0.0176,  0.6880,  0.1866,  ...,  0.6715, -0.9262,  1.5946]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "embedded_seq shape: torch.Size([1, 5, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "# 定义词汇表大小和词向量维度\n",
    "vocab_size = 10000\n",
    "d_model = 300\n",
    "\n",
    "# 创建嵌入层对象\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# 定义输入序列\n",
    "input_seq = torch.LongTensor([[1, 2, 3, 4, 5]])\n",
    "print(\"input_seq:\", input_seq)\n",
    "\n",
    "# 将输入序列传递给嵌入层\n",
    "embedded_seq = embedding(input_seq)\n",
    "print(\"embedded_seq:\\n\", embedded_seq)\n",
    "\n",
    "# 打印嵌入后的序列形状\n",
    "print(\"embedded_seq shape:\", embedded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "#定义一个简单的分词器\n",
    "tokenize = lambda x: x.split()\n",
    "\n",
    "source_field = Field(tokenize=tokenize, tokenizer_language='en', init_token='<sos>', eos_token='<eos>')\n",
    "target_field = Field(tokenize=tokenize, tokenizer_language='de', init_token='<sos>', eos_token='<eos>')\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.en', '.de'), fields=(source_field, target_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.']\n",
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system.']\n",
      "['Mehrere', 'Männer', 'mit', 'Schutzhelmen', 'bedienen', 'ein', 'Antriebsradsystem.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['A', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse.']\n",
      "['Ein', 'kleines', 'Mädchen', 'klettert', 'in', 'ein', 'Spielhaus', 'aus', 'Holz.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['A', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window.']\n",
      "['Ein', 'Mann', 'in', 'einem', 'blauen', 'Hemd', 'steht', 'auf', 'einer', 'Leiter', 'und', 'putzt', 'ein', 'Fenster.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Two', 'men', 'are', 'at', 'the', 'stove', 'preparing', 'food.']\n",
      "['Zwei', 'Männer', 'stehen', 'am', 'Herd', 'und', 'bereiten', 'Essen', 'zu.']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "for i in range(5):\n",
    "    print(train_data[i].src)\n",
    "    print(train_data[i].trg)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个sample的src从str转换成int:\n",
      " tensor([[   2,    5,   22,   15, 2848,    4, 4392,    6, 2113,    8,    7, 1919,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,    5,   15, 2179,    4, 2306,   25,   32,    6,    4, 1327,  310,\n",
      "           98,  113, 1976,    0, 1325,    0,   42,  191,    9,    0,   44,   47,\n",
      "           12,    7, 4201,    3],\n",
      "        [   2,    5,   11,   19,    4,  249,    9,   89,   23,  114,   39,    4,\n",
      "         1522,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,  189,    9,    4,   65,   73,    4,  437, 2245,    6,   40,   12,\n",
      "            4,  177,  700,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,   45,  118,   11,    6,  445,  144,   10, 1498, 3359, 2989,   39,\n",
      "            7,   68,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n",
      "第一个sample的src的shape:\n",
      " torch.Size([5, 28])\n",
      "将这个sample的src转回str:\n",
      "['<sos>', 'A', 'young', 'woman', 'writes', 'a', 'message', 'in', 'chalk', 'on', 'the', 'pavement.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'A', 'woman', 'opens', 'a', 'gift', 'while', 'standing', 'in', 'a', 'dining', 'room', 'that', 'has', 'been', '<unk>', 'decorated', '<unk>', 'her', 'face', 'is', '<unk>', 'by', 'one', 'of', 'the', 'decorations.', '<eos>']\n",
      "['<sos>', 'A', 'man', 'wearing', 'a', 'helmet', 'is', 'jumping', 'his', 'bike', 'down', 'a', 'staircase.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'There', 'is', 'a', 'person', 'riding', 'a', 'tall', 'bike,', 'in', 'front', 'of', 'a', 'purple', 'bus.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'An', 'Asian', 'man', 'in', 'short', 'shorts', 'and', 'knee', 'pads', 'jogs', 'down', 'the', 'street.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "source_field.build_vocab(train_data, min_freq=2)\n",
    "target_field.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=5,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "sample_data = next(iter(train_iterator))\n",
    "print(\"第一个sample的src从str转换成int:\\n\",sample_data.src.T)\n",
    "print(\"第一个sample的src的shape:\\n\", sample_data.src.T.shape)\n",
    "print(\"将这个sample的src转回str:\")\n",
    "for i in range(sample_data.src.T.shape[0]):\n",
    "    print([source_field.vocab.itos[i] for i in sample_data.src.T[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 29000\n",
      "验证集大小: 1014\n",
      "测试集大小: 1000\n",
      "英文词汇表大小: 7964\n",
      "德文词汇表大小: 9762\n"
     ]
    }
   ],
   "source": [
    "# 打印数据集信息\n",
    "print(f\"训练集大小: {len(train_data.examples)}\")\n",
    "print(f\"验证集大小: {len(valid_data.examples)}\")\n",
    "print(f\"测试集大小: {len(test_data.examples)}\")\n",
    "print(f\"英文词汇表大小: {len(source_field.vocab)}\")\n",
    "print(f\"德文词汇表大小: {len(target_field.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "从上文，我们获取了一个英德数据集，并且我们获取了二者的词汇表大小。并且在通过BucketIterator处理数据集之后，我们获取了三个元素。\n",
    "*  \\<sos>  \\<eos> \\<pad>  \n",
    "并且可以观察得出，sos对应的是2，eos对应的是3，pad对应的是1\n",
    "值得注意的是，统一batch中的sentence由于句长不同，我们做了pad，这也引入了后面的Padding Mask。  \n",
    "现在我们可以开始构建transformer的第一个组件了，InputEmbedding，OutputEmbedding。\n",
    "现在这个Transformer模型出生了:\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model):\n",
    "        #InputEmbedding\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, d_model)\n",
    "        #OutputEmbedding\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, d_model)\n",
    "```\n",
    "一开始长的简陋，但是我们一点点补充他的能力。那按照之前embedding的定义和数据集的情况  \n",
    "`input_dim=len(source_field.vocab)`  \n",
    "`output_dim=len(target_field.vocab)`\n",
    "\n",
    "d_modle这个变量会伴随整个transformer网络的定义过程。  \n",
    "模型的输入的shape为\\[batch, seq_len\\]  \n",
    "经过embedding之后变为\\[batch, seq_len, d_model\\]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2、位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 定义位置编码层\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / d_model)))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "结合位置编码的公式来看\n",
    "$$ PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}}) $$  \n",
    "$$ PE(pos, 2i + 1) = cos(pos / 10000^{2i/d_{model}}) $$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3、Attention is all you need\n",
    "说完数据的embedding和位置编码，这个章节我们会介绍Self-Attention，MultiHeadAttention以及Decoder中的Mask-Multi-Head-Attention。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1、Self-Attention\n",
    "\n",
    "#### Step 1\n",
    "计算QKV\n",
    "![](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ q_{1} = x_{1} * W^{Q} $$  \n",
    "$$ k_{1} = x_{1} * W^{K} $$  \n",
    "$$ v_{1} = x_{1} * W^{V} $$  \n",
    "$$ q_{2} = x_{2} * W^{Q} $$  \n",
    "$$ k_{2} = x_{2} * W^{K} $$  \n",
    "$$ v_{2} = x_{2} * W^{V} $$  \n",
    "\n",
    "> 可以注意到，上述过程中，不同x分享了同一个W,通过这个操作，和已经发生了某种程度上的信息交换。也就是说，单词和单词之间，通过共享权值，已经相互发生了信息的交换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "到这里，我们的SelfAttenion可以增加三个组件了,并且forward可以初步定义了：\n",
    "```\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step2\n",
    "计算一个“score”\n",
    "![](https://jalammar.github.io/images/t/transformer_self_attention_score.png)\n",
    "> 假设我们正在计算本例中第一个单词“Thinking”的自注意力。我们需要根据输入句子的每个单词对这个单词进行评分。当我们在某个位置对单词进行编码时，分数决定了对输入句子的其他部分的关注程度。（机翻）\n",
    "\n",
    "$$ score_{1} = q_{1} * k_{1}^{T} $$  \n",
    "$$ score_{2} = q_{2} * k_{2}^{T} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "到这里，我们的SelfAttenion继续进化:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step2-3\n",
    "计算由“score”组成的Attention Weight\n",
    "\n",
    "![](https://jalammar.github.io/images/t/self-attention_softmax.png)  \n",
    "   \n",
    "     \n",
    "$$ attention\\_weight =  softmax(score/\\sqrt{8})$$  \n",
    "这里的8是一个经验值，我们重实现，不重炼丹，忽略。\n",
    "到这里，我们的SelfAttenion继续进化:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # 计算Attention Weight\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8)), dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step4\n",
    "与V联合，计算出一个“Context”\n",
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最终的SelfAttenion如下:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # 计算Attention Weight\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8)), dim=-1)\n",
    "        \n",
    "        #计算context\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(q)\n",
    "        print(\"query shape:\", query.shape)\n",
    "        key = self.key(v)\n",
    "        print(\"key shape:\", key.shape)\n",
    "        value = self.value(v)\n",
    "        print(\"value shape:\", value.shape)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        print(\"scores shape:\", scores.shape)\n",
    "        print(f\"score的shape含义代表，这{q.size(0)}个句子里，每个token和其他同在一条语句中的token之间的联系\")\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8.0)), dim=-1)\n",
    "        print(\"attn_weights shape:\", attn_weights.shape)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        print(\"context shape:\", context.shape)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们来用上文中获取到的`sampel_data`来跑一个SelfAttention，加深下理解,之前数据集的信息\n",
    "```\n",
    "第一个数据集从str转换成int: tensor([[   2],\n",
    "        [  43],\n",
    "        [  15],\n",
    "        [ 624],\n",
    "        [   4],\n",
    "        [2306],\n",
    "        [  76],\n",
    "        [  12],\n",
    "        [   4],\n",
    "        [2114],\n",
    "        [   0],\n",
    "        [   3]])\n",
    "训练集大小: 29000\n",
    "验证集大小: 1014\n",
    "测试集大小: 1000\n",
    "英文词汇表大小: 7964\n",
    "德文词汇表大小: 9762\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入shape是torch.Size([5, 28]),代表的是说一共有5个句子，每个句子被pad成了28的长度\n",
      "输入长这样：\n",
      " tensor([[   2,    5,   22,   15, 2848,    4, 4392,    6, 2113,    8,    7, 1919,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,    5,   15, 2179,    4, 2306,   25,   32,    6,    4, 1327,  310,\n",
      "           98,  113, 1976,    0, 1325,    0,   42,  191,    9,    0,   44,   47,\n",
      "           12,    7, 4201,    3],\n",
      "        [   2,    5,   11,   19,    4,  249,    9,   89,   23,  114,   39,    4,\n",
      "         1522,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,  189,    9,    4,   65,   73,    4,  437, 2245,    6,   40,   12,\n",
      "            4,  177,  700,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2,   45,  118,   11,    6,  445,  144,   10, 1498, 3359, 2989,   39,\n",
      "            7,   68,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n",
      "通过词嵌入后，shape变成了torch.Size([5, 28, 300])，这代表一共有5个句子,\n",
      " 每个句子有28个token，每个token被映射到一个300维空间\n",
      "经过了位置编码后，shape不变，依然为torch.Size([5, 28, 300])\n",
      "query shape: torch.Size([5, 28, 300])\n",
      "key shape: torch.Size([5, 28, 300])\n",
      "value shape: torch.Size([5, 28, 300])\n",
      "scores shape: torch.Size([5, 28, 28])\n",
      "score的shape含义代表，这5个句子里，每个token和其他同在一条语句中的token之间的联系\n",
      "attn_weights shape: torch.Size([5, 28, 28])\n",
      "context shape: torch.Size([5, 28, 300])\n",
      "最终输出shape torch.Size([5, 28, 300])，与q，k，v shape相同\n"
     ]
    }
   ],
   "source": [
    "sample_data\n",
    "# 英文词汇表大小: 7964，我们设置d_model为300\n",
    "sample_embedding = nn.Embedding(7964, 300)\n",
    "e = sample_data.src.T\n",
    "print(f\"输入shape是{e.shape},代表的是说一共有{e.size(0)}个句子，每个句子被pad成了{e.size(1)}的长度\")\n",
    "print(\"输入长这样：\\n\", e)\n",
    "e = sample_embedding(e)\n",
    "# print(e) \n",
    "print(f\"通过词嵌入后，shape变成了{e.shape}，这代表一共有{e.size(0)}个句子,\\n\",\n",
    "      f\"每个句子有{e.size(1)}个token，每个token被映射到一个{e.size(2)}维空间\")\n",
    "    \n",
    "sample_pe = PositionalEncoding(300)\n",
    "e = sample_pe(e)\n",
    "# print(e)\n",
    "print(f\"经过了位置编码后，shape不变，依然为{e.shape}\")\n",
    "\n",
    "sample_sa = SelfAttention(300)\n",
    "\n",
    "e, _ = sample_sa(e, e, e)\n",
    "# print(e)\n",
    "print(f\"最终输出shape {e.shape}，与q，k，v shape相同\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "可以看到上文sample表示的网络流程如下，最终Z则是我们输出的context  \n",
    "![](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过embedding和position encoding之后，self attention的输入为\\[batch, seq_len, d_model\\]  \n",
    "query的shape为\\[batch, seq_len, d_model\\], key, value的shape与query相同  \n",
    "score的shape为\\[batch, seq_len, seq_len\\]  \n",
    "attn_weights的shape为\\[batch, seq_len, seq_len\\],与score的shape相同  \n",
    "context的shape为\\[batch, seq_len, d_model\\]，与query，key，value的shape相同\n",
    "\n",
    "1、$ Q \\times K^{T}$    \n",
    "$ [batch, seq\\_len, d\\_model] \\times [batch, d\\_model, seq\\_len] = [batch, seq\\_len, seq\\_len]$  \n",
    "\n",
    "\n",
    "2、$ softmax(\\frac{Q \\times K^{T}}{\\sqrt(d_k)})$   \n",
    "$[batch, seq\\_len, seq\\_len]$\n",
    "\n",
    "\n",
    "3、$ softmax(\\frac{Q \\times K^{T}}{\\sqrt dk}) \\times V $  \n",
    "$[batch, seq\\_len, seq\\_len] \\times [batch, seq\\_len, d\\_model] = [batch, seq\\_len, d\\_model]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2、multi-headed Attention\n",
    "所谓多头注意力机制，即多个SelfAttention模块组合，这样存在多份Q，K，V权重矩阵。我们也可以很粗浅的认为，这样可以从不同的视角去理解一个句子。 如下图所示，一个8头注意力机制可以产生8个$Z(context)$矩阵。但是对于Transformer中的feedforward模块，并不想感知多个矩阵，于是就通过下图的方法，对于多头注意力，网络中添加了一个$W^{O}$，用于将concat起来之后的$[Z_{0}, Z_{1}...Z_{7}]$进行变换为$Z$。  \n",
    "\n",
    "在了解SelfAttention的实现之后，我们发现multi-headed Attention的思想也非常朴素。所以我们可以开始实现一个multi-headed Attention。\n",
    "![](https://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n",
    "![](https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"Hidden dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(d_model, d_model) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(num_heads * self.head_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_outputs = []\n",
    "        \n",
    "        for attention_head in self.attention_heads:\n",
    "            context, _ = attention_head(x)\n",
    "            attention_outputs.append(context)\n",
    "        \n",
    "        # 将多个注意力头的输出拼接在一起\n",
    "        concatenated = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # 使用线性层进行投影\n",
    "        output = self.fc(concatenated)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "以上是一个MultiHeadAttention的简单的直观实现，这个实现是通过做head_num次SelfAttention来去获取head_num个shape为$[batch, d\\_model, d\\_model/num\\_heads]$的context，最终合并成shape为$[batch, d\\_model, d\\_model]$的context。  \n",
    "这样效率是比较低下的。以下是一个常见的工程实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_num, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.head_num = head_num\n",
    "        \n",
    "        # d_model // h 仍然是要能整除，换个名字仍然意义不变\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        \n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bsz = query.shape[0]\n",
    "\n",
    "        query = self.q(q)\n",
    "        key = self.k(k)\n",
    "        value = self.v(v)\n",
    "\n",
    "        query = query.view(bsz, -1, self.head_num, self.d_model //self.head_num).transpose(1, 2)\n",
    "        key = key.view(bsz, -1, self.head_num, self.d_model //self.head_num).transpose(1, 2)\n",
    "        value = value.view(bsz, -1, self.head_num, self.d_model //self.head_num).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) \n",
    "        \n",
    "        attn_weights = self.dropout(torch.softmax(energy, dim=-1))\n",
    "\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        context = context.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "\n",
    "        context = self.fc(context)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q: \\ [batch, seq\\_len, d\\_model] \\ as \\ k \\ v$  \n",
    "\n",
    "\n",
    "$query: \\ [batch, seq\\_len, d\\_model] \\ as \\ key \\ value$  \n",
    "\n",
    "\n",
    "`query.view(bsz, -1, self.head_num, self.d_model //self.head_num) `  \n",
    "$query \\ update \\ as: \\ [batch, seq\\_len, head\\_num, d\\_model \\div head_num]$  \n",
    "\n",
    "\n",
    "`query = query.view(bsz, -1, self.head_num, self.d_model //self.head_num).transpose(1, 2)`  \n",
    "$query \\ update \\ as: \\ [batch, head\\_num, seq\\_len, d\\_model \\div head\\_num] \\ as \\ key \\ value$  \n",
    "\n",
    "\n",
    "`scores = torch.matmul(query, key.permute(0, 1, 3, 2))`  \n",
    "$[batch, head\\_num, seq\\_len, d\\_model \\div head\\_num] \\times [batch, head\\_num, d\\_model \\div head\\_num, seq\\_len]$  \n",
    "$scores：[batch, head\\_num, seq\\_len, seq\\_len]$  \n",
    "\n",
    "\n",
    "`context = torch.matmul(attn_weights, V)`  \n",
    "$[batch, head\\_num, seq\\_len, seq\\_len] \\times [batch, head\\_num, seq\\_len, d\\_model \\div head\\_num]$   \n",
    "$context：[batch, head\\_num, seq\\_len, d\\_model \\div head\\_num]$  \n",
    "\n",
    "\n",
    "`context = context.permute(0, 2, 1, 3).contiguous()`  \n",
    "$context  \\ update \\ as: [batch, seq\\_len, head\\_num, d\\_model \\div head\\_num]$  \n",
    "\n",
    "\n",
    "`context = context.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads)`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3、Mask-Multi-Head-Attention\n",
    "我们先不着急来取看Mask-Multi-Head-Attention，移步到4章节，我们先看下encoder实现。\n",
    "\n",
    "\n",
    "> mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。   \n",
    "> Padding Mask\n",
    "什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n",
    "具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。  \n",
    "> Sequence mask\n",
    "文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。\n",
    "那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n",
    "\n",
    "以下是一个非常直观的mask实现: $e^{-inf} =0$,这样score对于mask为0的token的注意力就为0，那至于是Padding Mask还是Sequence mask，我们只需要控制传入的mask参数就好了。\n",
    "```python\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        # 在mask为0的地方填充负无穷\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, value)\n",
    "        return context, attention_weights\n",
    "```\n",
    "那我们就可以稍稍改变下SelfAttention的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 计算query、key、value\n",
    "        print(x.device)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8.0)), dim=-1)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4、Encoder\n",
    "我们来看下Encoder还差哪些元素，实现了MHA后，我们还需要FeedForward模块和Add & Norm模块，其中的虚线代表的是残差连接。（残差思想见Resnet50），这里的Normalize使用的是LayerNorm而非BatchNorm，至于二者的差别，大家可以自己扩展学习下。这里我们注重实现。\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        self.attention_layer = MultiHeadAttention(d_model, d_model, num_heads) \n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward_layer = FeedForward(d_model, d_model, 0.1) \n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # MultiHeadAttention\n",
    "        attention_output = self.attention_layer(x)\n",
    "        # Add + Normalize\n",
    "        attention_output = self.attention_norm(attention_output + x)\n",
    "        encoded = attention_output\n",
    "\n",
    "        # FeedForward\n",
    "        feedforward_output = self.feedforward_layer(x)\n",
    "        # Add + Normalize\n",
    "        feedforward_output = self.feedforward_norm(feedforward_output + x)\n",
    "        \n",
    "        return feedforward_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "至此，我们就实现了Transformer中的Encoder模块，Transformer的定义已经完成了50%。在Embedding组件的基础上，我们又增加了Encoder组件。\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model):\n",
    "        #InputEmbedding\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, d_model)\n",
    "        #OutputEmbedding\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, d_model)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(input_dim, d_model, num_heads, num_layers)\n",
    "```\n",
    "![](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)\n",
    "\n",
    "让我们来基于encoder来跑一个demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([5, 19, 320])\n",
      "cpu\n",
      "cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 5, 19, 160]          51,360\n",
      "            Linear-2           [-1, 5, 19, 160]          51,360\n",
      "            Linear-3           [-1, 5, 19, 160]          51,360\n",
      "     SelfAttention-4  [[-1, 5, 19, 160], [-1, 5, 19, 19]]               0\n",
      "            Linear-5           [-1, 5, 19, 160]          51,360\n",
      "            Linear-6           [-1, 5, 19, 160]          51,360\n",
      "            Linear-7           [-1, 5, 19, 160]          51,360\n",
      "     SelfAttention-8  [[-1, 5, 19, 160], [-1, 5, 19, 19]]               0\n",
      "            Linear-9           [-1, 5, 19, 320]         102,720\n",
      "MultiHeadAttention-10           [-1, 5, 19, 320]               0\n",
      "        LayerNorm-11           [-1, 5, 19, 320]             640\n",
      "           Linear-12           [-1, 5, 19, 320]         102,720\n",
      "          Dropout-13           [-1, 5, 19, 320]               0\n",
      "           Linear-14           [-1, 5, 19, 320]         102,720\n",
      "      FeedForward-15           [-1, 5, 19, 320]               0\n",
      "        LayerNorm-16           [-1, 5, 19, 320]             640\n",
      "================================================================\n",
      "Total params: 617,600\n",
      "Trainable params: 617,600\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 416.09\n",
      "Params size (MB): 2.36\n",
      "Estimated Total Size (MB): 418.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "e = sample_data.src.T.contiguous()\n",
    "print(e.device)\n",
    "sample_embedding = nn.Embedding(7964, 320)\n",
    "e = sample_embedding(e)\n",
    "print(e.shape)\n",
    "# 英文词汇表大小: 7964，我们设置embeding dim为320, 8头注意力\n",
    "sample_encoder = EncoderLayer(7964, 320, 2).cpu()\n",
    "# sample_encoder(e).shape\n",
    "summary(sample_encoder, e.shape, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5、Decoder\n",
    "从下图来看，我们可以看到Decoder的构造和Encoder的差别主要是Encoder-Decoder Attention，但是这张图没有把最下面的Self-Attention模块画准确。这个Self-Attention其实就是我们上文的Mask-Multi-Head-Attention。\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "3.3说完Mask-Multi-Head-Attention，我们继续看Encoder-Decoder Attention。\n",
    "Encoder-Decoder Attention与Self-Attention的最大差别其实就是Attention中的Query来自解码器，而Key和Value来自编码器。上图中encoder和decoder连接的虚线就是代表着这个差别。这样我们又可以进化一下我们的Self-Attention的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6、End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "    def forward(self, input_q, input_k, input_v, mask=None):\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = input_q.size(1)\n",
    "        \n",
    "        # 计算query、key、value\n",
    "        query = self.query(input_q)\n",
    "        key = self.key(input_k)\n",
    "        value = self.value(input_v)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == False, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(self.d_model)), dim=-1)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"Hidden dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(input_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(num_heads * self.head_dim, input_dim)\n",
    "        \n",
    "    def forward(self, input_q, input_k, input_v, mask=None):\n",
    "        attention_outputs = []\n",
    "        \n",
    "        for attention_head in self.attention_heads:\n",
    "            context, _ = attention_head(input_q, input_k, input_v, mask)\n",
    "            attention_outputs.append(context)\n",
    "        \n",
    "        # 将多个注意力头的输出拼接在一起\n",
    "        concatenated = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # 使用线性层进行投影\n",
    "        output = self.fc(concatenated)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.attention_layer = MultiHeadAttention(d_model, d_model, num_heads) \n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward_layer = FeedForward(d_model, d_model, 0.1) \n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        # MultiHeadAttention\n",
    "        attention_output = self.attention_layer(x, x, x, mask)\n",
    "        # Add + Normalize\n",
    "        attention_output = self.attention_norm(attention_output + x)\n",
    "\n",
    "        # FeedForward\n",
    "        feedforward_output = self.feedforward_layer(attention_output)\n",
    "        # Add + Normalize\n",
    "        feedforward_output = self.feedforward_norm(feedforward_output + attention_output)\n",
    "        encoded = feedforward_output\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, d_model, num_heads)\n",
    "        self.encoder_attention = MultiHeadAttention(d_model, d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_model, 0.1)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
    "        self_attention_output = self.self_attention(x, x, x, target_mask)\n",
    "        x = x + self_attention_output\n",
    "        x = self.norm(x)\n",
    "\n",
    "        encoder_attention_output = self.encoder_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = x + encoder_attention_output\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, target_mask)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, d_model, num_heads, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model)\n",
    "        self.encoder = Encoder(d_model, num_heads, num_layers)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_layers)\n",
    "        self.output_linear = nn.Linear(d_model, output_vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq, input_mask, target_mask):\n",
    "        encoder_output = self.encode(input_seq, input_mask)\n",
    "        decoder_output = self.decode(target_seq, encoder_output, input_mask, target_mask)\n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output\n",
    "\n",
    "    def encode(self, input_seq, input_mask):\n",
    "        embedded_input = self.pos_embedding(self.input_embedding(input_seq))\n",
    "        encoder_output = self.encoder(embedded_input, input_mask)\n",
    "        return encoder_output\n",
    "\n",
    "    def decode(self, target_seq, encoder_output, input_mask, target_mask):\n",
    "        embedded_target = self.pos_embedding(self.output_embedding(target_seq))\n",
    "        decoder_output = self.decoder(embedded_target, encoder_output, input_mask, target_mask)\n",
    "        return decoder_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'young,',\n",
       " 'White',\n",
       " 'males',\n",
       " 'are',\n",
       " 'outside',\n",
       " 'near',\n",
       " 'many',\n",
       " 'bushes.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src\n",
    "train_data[0].src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们来定义几个参数\n",
    "\n",
    "'''\n",
    "训练集大小: 29000\n",
    "验证集大小: 1014\n",
    "测试集大小: 1000\n",
    "英文词汇表大小: 7964\n",
    "德文词汇表大小: 9762\n",
    "'''\n",
    "\n",
    "'''\n",
    "在Transformer模型中，d_model是一个超参数，代表模型中隐藏状态的维度或特征的维度。它是指输入、输出和内部表示中的向量维度。\n",
    "\n",
    "在Transformer的编码器和解码器中，所有的嵌入层、注意力机制、前馈神经网络等层的输入和输出向量的维度都是d_model。\n",
    "\n",
    "这个超参数的选择对于Transformer模型的性能和表示能力具有重要影响。较大的d_model可以提供更丰富的特征表示能力，但也会增加模型的计算和存储成本。通常情况下，较常见的d_model取值为128、256、512等。实际中，选择合适的d_model大小需要根据任务的复杂性和数据集的大小来进行调整。\n",
    "\n",
    "需要注意的是，d_model的值必须在整个Transformer模型中保持一致，以确保输入和输出的一致性，并且层之间的连接和参数共享都能够正确地进行。\n",
    "'''\n",
    "\n",
    "input_vocab_size = 7964\n",
    "output_vocab_size = 9762\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "model = Transformer(input_vocab_size, output_vocab_size, d_model, num_heads, num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape： torch.Size([5, 19])\n",
      "trg shape： torch.Size([5, 17])\n",
      "torch.Size([5, 16, 9762])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(padded_input):\n",
    "    #注意，这里的1代表的是pad，这里写死。\n",
    "    return (padded_input != 1).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_attn_subsequence_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=5,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model.train()\n",
    "for item in train_iterator:\n",
    "    src = item.src.T\n",
    "    trg = item.trg.T\n",
    "    \n",
    "    no_eos_target = trg[:,:-1]\n",
    "    print(\"src shape：\", src.shape)\n",
    "    print(\"trg shape：\", trg.shape)\n",
    "    src_mask = get_attn_pad_mask(src)\n",
    "    out_mask = get_attn_pad_mask(no_eos_target) & get_attn_subsequence_mask(no_eos_target.size(-1))\n",
    "    x = model(src, no_eos_target, src_mask, out_mask)\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
