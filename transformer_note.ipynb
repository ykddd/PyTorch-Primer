{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer网络结构解读及Pytorch简单实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1、前言\n",
    "transformer作为大模型基础的网络架构，了解其中的实现原理是AI从业人员的基础要求。本文借助网络资料及Chat-gpt，基于Pytorch实现了一个基本的Transform模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"700\"\n",
       "            src=\"https://arxiv.org/pdf/1706.03762\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1e6e3275630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1706.03762', width=1000, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2、词向量及位置编码\n",
    "一般而言，Transformer的介绍基本从Attention机制说起，但是本文的思路是构造一个端到端的Transformer模型用于训练，所以我们先从模型的输入开始。\n",
    "\n",
    "### 2.1、词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq: tensor([[1, 2, 3, 4, 5]])\n",
      "embedded_seq: tensor([[[ 1.0484,  0.0552, -1.2438,  ...,  0.4378,  1.1595,  0.5963],\n",
      "         [ 0.3047, -0.0406,  0.2524,  ..., -1.0038,  2.2379,  0.2897],\n",
      "         [-0.5419, -0.4677, -0.1212,  ...,  0.7968,  1.1909, -0.7909],\n",
      "         [ 0.8764,  0.4880,  2.0541,  ..., -0.5178,  0.2409, -1.5589],\n",
      "         [ 0.3369,  0.2621,  0.6169,  ..., -0.3548,  1.0849,  2.6579]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "embedded_seq shape: torch.Size([1, 5, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# 定义词汇表大小和词向量维度\n",
    "vocab_size = 10000\n",
    "embedding_dim = 300\n",
    "\n",
    "# 创建嵌入层对象\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 定义输入序列\n",
    "input_seq = torch.LongTensor([[1, 2, 3, 4, 5]])\n",
    "print(\"input_seq:\", input_seq)\n",
    "\n",
    "# 将输入序列传递给嵌入层\n",
    "embedded_seq = embedding(input_seq)\n",
    "print(\"embedded_seq:\", embedded_seq)\n",
    "\n",
    "# 打印嵌入后的序列形状\n",
    "print(\"embedded_seq shape:\", embedded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "#定义一个简单的分词器\n",
    "tokenize = lambda x: x.split()\n",
    "\n",
    "source_field = Field(tokenize=tokenize, tokenizer_language='en', init_token='<sos>', eos_token='<eos>')\n",
    "target_field = Field(tokenize=tokenize, tokenizer_language='de', init_token='<sos>', eos_token='<eos>')\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.en', '.de'), fields=(source_field, target_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'young,', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes.']\n",
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system.']\n",
      "['Mehrere', 'Männer', 'mit', 'Schutzhelmen', 'bedienen', 'ein', 'Antriebsradsystem.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['A', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse.']\n",
      "['Ein', 'kleines', 'Mädchen', 'klettert', 'in', 'ein', 'Spielhaus', 'aus', 'Holz.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['A', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window.']\n",
      "['Ein', 'Mann', 'in', 'einem', 'blauen', 'Hemd', 'steht', 'auf', 'einer', 'Leiter', 'und', 'putzt', 'ein', 'Fenster.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Two', 'men', 'are', 'at', 'the', 'stove', 'preparing', 'food.']\n",
      "['Zwei', 'Männer', 'stehen', 'am', 'Herd', 'und', 'bereiten', 'Essen', 'zu.']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "for i in range(5):\n",
    "    print(train_data[i].src)\n",
    "    print(train_data[i].trg)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个sample的src从str转换成int:  tensor([[   2,   17,   20,   97,   77,    4,   30, 2843,    3,    1,    1,    1,\n",
      "            1,    1],\n",
      "        [   2,   50,   22,   36,  191,   24,  103,   11,   13,    4, 4714,    3,\n",
      "            1,    1],\n",
      "        [   2,    5,   54,   33,   94,   18,   23, 3400,    6,    4,    0, 3007,\n",
      "          188,    3],\n",
      "        [   2,    5,   11,    9,    8,    4,  329,    8,    4,   30,  762,    3,\n",
      "            1,    1],\n",
      "        [   2,   43,    0,  139,   11, 2135,  779,    6,    7,  701,    3,    1,\n",
      "            1,    1]])\n",
      "第一个sample的src的shape:  torch.Size([5, 14])\n",
      "将这个sample的src转回str:\n",
      "['<sos>', 'Two', 'people', 'stand', 'near', 'a', 'red', 'wheelbarrow.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'Three', 'young', 'men', 'face', 'an', 'older', 'man', 'with', 'a', 'whistle.', '<eos>', '<pad>', '<pad>']\n",
      "['<sos>', 'A', 'little', 'boy', 'looks', 'at', 'his', 'reflection', 'in', 'a', '<unk>', 'marble', 'wall.', '<eos>']\n",
      "['<sos>', 'A', 'man', 'is', 'on', 'a', 'skateboard', 'on', 'a', 'red', 'ramp.', '<eos>', '<pad>', '<pad>']\n",
      "['<sos>', 'The', '<unk>', 'old', 'man', 'falls', 'asleep', 'in', 'the', 'chair.', '<eos>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "source_field.build_vocab(train_data, min_freq=2)\n",
    "target_field.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=5,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "sample_data = next(iter(train_iterator))\n",
    "print(\"第一个sample的src从str转换成int: \", sample_data.src.T)\n",
    "print(\"第一个sample的src的shape: \", sample_data.src.T.shape)\n",
    "print(\"将这个sample的src转回str:\")\n",
    "for i in range(sample_data.src.T.shape[0]):\n",
    "    print([source_field.vocab.itos[i] for i in sample_data.src.T[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 29000\n",
      "验证集大小: 1014\n",
      "测试集大小: 1000\n",
      "英文词汇表大小: 7964\n",
      "德文词汇表大小: 9762\n"
     ]
    }
   ],
   "source": [
    "# 打印数据集信息\n",
    "print(f\"训练集大小: {len(train_data.examples)}\")\n",
    "print(f\"验证集大小: {len(valid_data.examples)}\")\n",
    "print(f\"测试集大小: {len(test_data.examples)}\")\n",
    "print(f\"英文词汇表大小: {len(source_field.vocab)}\")\n",
    "print(f\"德文词汇表大小: {len(target_field.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "从上文，我们获取了一个英德数据集，并且我们获取了二者的词汇表大小。并且在通过BucketIterator处理数据集之后，我们获取了三个元素。\n",
    "*  \\<sos>  \\<eos> \\<pad>  \n",
    "值得注意的是，统一batch中的sentence由于句长不同，我们做了pad，这也引入了后面的Padding Mask。  \n",
    "现在我们可以开始构建transformer的第一个组件了，InputEmbedding，OutputEmbedding。\n",
    "现在这个Transformer模型出生了:\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model):\n",
    "        #InputEmbedding\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, d_model)\n",
    "        #OutputEmbedding\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, d_model)\n",
    "```\n",
    "一开始长的简陋，但是我们一点点补充他的能力。那按照之前embedding的定义和数据集的情况  \n",
    "`input_dim=len(source_field.vocab)`  \n",
    "`output_dim=len(target_field.vocab)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2、位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 定义位置编码层 TODO\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3、Attention is all you need\n",
    "说完数据的embedding和位置编码，这个章节我们会介绍Self-Attention，MultiHeadAttention以及Decoder中的Mask-Multi-Head-Attention。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1、Self-Attention\n",
    "\n",
    "#### Step 1\n",
    "计算QKV\n",
    "![](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ q_{1} = x_{1} * W^{Q} $$  \n",
    "$$ k_{1} = x_{1} * W^{K} $$  \n",
    "$$ v_{1} = x_{1} * W^{V} $$  \n",
    "$$ q_{2} = x_{2} * W^{Q} $$  \n",
    "$$ k_{2} = x_{2} * W^{K} $$  \n",
    "$$ v_{2} = x_{2} * W^{V} $$  \n",
    "\n",
    "> 可以注意到，上述过程中，不同x分享了同一个W,通过这个操作，和已经发生了某种程度上的信息交换。也就是说，单词和单词之间，通过共享权值，已经相互发生了信息的交换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "到这里，我们的SelfAttenion可以增加三个组件了,并且forward可以初步定义了：\n",
    "```\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step2\n",
    "计算一个“score”\n",
    "![](https://jalammar.github.io/images/t/transformer_self_attention_score.png)\n",
    "> 假设我们正在计算本例中第一个单词“Thinking”的自注意力。我们需要根据输入句子的每个单词对这个单词进行评分。当我们在某个位置对单词进行编码时，分数决定了对输入句子的其他部分的关注程度。（机翻）\n",
    "\n",
    "$$ score_{1} = q_{1} * k_{1}^{T} $$  \n",
    "$$ score_{2} = q_{2} * k_{2}^{T} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "到这里，我们的SelfAttenion继续进化:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step2-3\n",
    "计算由“score”组成的Attention Weight\n",
    "\n",
    "![](https://jalammar.github.io/images/t/self-attention_softmax.png)  \n",
    "   \n",
    "     \n",
    "$$ attention\\_weight =  softmax(score/\\sqrt{8})$$  \n",
    "这里的8是一个经验值，我们重实现，不重炼丹，忽略。\n",
    "到这里，我们的SelfAttenion继续进化:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # 计算Attention Weight\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8)), dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step4\n",
    "与V联合，计算出一个“Context”\n",
    "![](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最终的SelfAttenion如下:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # 计算Attention Weight\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8)), dim=-1)\n",
    "        \n",
    "        #计算context\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 计算query、key、value\n",
    "        query = self.query(x)\n",
    "        print(\"query shape:\", query.shape)\n",
    "        key = self.key(x)\n",
    "        print(\"key shape:\", key.shape)\n",
    "        value = self.value(x)\n",
    "        print(\"value shape:\", value.shape)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        print(\"scores shape:\", scores.shape)\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8.0)), dim=-1)\n",
    "        print(\"attn_weights shape:\", attn_weights.shape)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        print(\"context shape:\", context.shape)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们来用上文中获取到的`sampel_data`来跑一个SelfAttention，加深下理解,之前数据集的信息\n",
    "```\n",
    "第一个数据集从str转换成int: tensor([[   2],\n",
    "        [  43],\n",
    "        [  15],\n",
    "        [ 624],\n",
    "        [   4],\n",
    "        [2306],\n",
    "        [  76],\n",
    "        [  12],\n",
    "        [   4],\n",
    "        [2114],\n",
    "        [   0],\n",
    "        [   3]])\n",
    "训练集大小: 29000\n",
    "验证集大小: 1014\n",
    "测试集大小: 1000\n",
    "英文词汇表大小: 7964\n",
    "德文词汇表大小: 9762\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 5])\n",
      "tensor([[   2,    2,    2,    2,    2],\n",
      "        [  17,   50,    5,    5,   43],\n",
      "        [  20,   22,   54,   11,    0],\n",
      "        [  97,   36,   33,    9,  139],\n",
      "        [  77,  191,   94,    8,   11],\n",
      "        [   4,   24,   18,    4, 2135],\n",
      "        [  30,  103,   23,  329,  779],\n",
      "        [2843,   11, 3400,    8,    6],\n",
      "        [   3,   13,    6,    4,    7],\n",
      "        [   1,    4,    4,   30,  701],\n",
      "        [   1, 4714,    0,  762,    3],\n",
      "        [   1,    3, 3007,    3,    1],\n",
      "        [   1,    1,  188,    1,    1],\n",
      "        [   1,    1,    3,    1,    1]])\n",
      "1 torch.Size([5, 14])\n",
      "e tensor([[   2,   17,   20,   97,   77,    4,   30, 2843,    3,    1,    1,    1,\n",
      "            1,    1],\n",
      "        [   2,   50,   22,   36,  191,   24,  103,   11,   13,    4, 4714,    3,\n",
      "            1,    1],\n",
      "        [   2,    5,   54,   33,   94,   18,   23, 3400,    6,    4,    0, 3007,\n",
      "          188,    3],\n",
      "        [   2,    5,   11,    9,    8,    4,  329,    8,    4,   30,  762,    3,\n",
      "            1,    1],\n",
      "        [   2,   43,    0,  139,   11, 2135,  779,    6,    7,  701,    3,    1,\n",
      "            1,    1]])\n",
      "2 torch.Size([5, 14, 300])\n",
      "3 torch.Size([5, 14, 300])\n",
      "query shape: torch.Size([5, 14, 400])\n",
      "key shape: torch.Size([5, 14, 400])\n",
      "value shape: torch.Size([5, 14, 400])\n",
      "scores shape: torch.Size([5, 14, 14])\n",
      "attn_weights shape: torch.Size([5, 14, 14])\n",
      "context shape: torch.Size([5, 14, 400])\n",
      "4 torch.Size([5, 14, 400])\n"
     ]
    }
   ],
   "source": [
    "sample_data\n",
    "print(sample_data.src.shape)\n",
    "print(sample_data.src)\n",
    "# 英文词汇表大小: 7964，我们设置embeding dim为300\n",
    "sample_embedding = nn.Embedding(7964, 300)\n",
    "e = sample_data.src.T\n",
    "# print(\"1\"e) \n",
    "print(\"1\", e.shape)\n",
    "print(\"e\", e)\n",
    "e = sample_embedding(e)\n",
    "# print(e) \n",
    "print(\"2\", e.shape)\n",
    "    \n",
    "sample_pe = PositionalEncoding(300)\n",
    "e = sample_pe(e)\n",
    "# print(e)\n",
    "print(\"3\", e.shape)\n",
    "\n",
    "sample_sa = SelfAttention(300, 400)\n",
    "\n",
    "e, _ = sample_sa(e)\n",
    "# print(e)\n",
    "print(\"4\", e.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "可以看到上文sample表示的网络流程如下，最终Z则是我们输出的context  \n",
    "![](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2、multi-headed Attention\n",
    "所谓多头注意力机制，即多个SelfAttention模块组合，这样存在多份Q，K，V权重矩阵。我们也可以很粗浅的认为，这样可以从不同的视角去理解一个句子。 如下图所示，一个8头注意力机制可以产生8个$Z(context)$矩阵。但是对于Transformer中的feedforward模块，并不想感知多个矩阵，于是就通过下图的方法，对于多头注意力，网络中添加了一个$W^{O}$，用于将concat起来之后的$[Z_{0}, Z_{1}...Z_{7}]$进行变换为$Z$。  \n",
    "\n",
    "在了解SelfAttention的实现之后，我们发现multi-headed Attention的思想也非常朴素。所以我们可以开始实现一个multi-headed Attention。\n",
    "![](https://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n",
    "![](https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"Hidden dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(input_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(num_heads * self.head_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_outputs = []\n",
    "        \n",
    "        for attention_head in self.attention_heads:\n",
    "            context, _ = attention_head(x)\n",
    "            attention_outputs.append(context)\n",
    "        \n",
    "        # 将多个注意力头的输出拼接在一起\n",
    "        concatenated = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # 使用线性层进行投影\n",
    "        output = self.fc(concatenated)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3、Mask-Multi-Head-Attention\n",
    "我们先不着急来取看Mask-Multi-Head-Attention，移步到4章节，我们先看下encoder实现。\n",
    "\n",
    "\n",
    "> mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。   \n",
    "> Padding Mask\n",
    "什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n",
    "具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。  \n",
    "> Sequence mask\n",
    "文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。\n",
    "那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n",
    "\n",
    "以下是一个非常直观的mask实现: $e^{-inf} =0$,这样score对于mask为0的token的注意力就为0，那至于是Padding Mask还是Sequence mask，我们只需要控制传入的mask参数就好了。\n",
    "```python\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        # 在mask为0的地方填充负无穷\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, value)\n",
    "        return context, attention_weights\n",
    "```\n",
    "那我们就可以稍稍改变下SelfAttention的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 计算query、key、value\n",
    "        print(x.device)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(8.0)), dim=-1)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4、Encoder\n",
    "我们来看下Encoder还差哪些元素，实现了MHA后，我们还需要FeedForward模块和Add & Norm模块，其中的虚线代表的是残差连接。（残差思想见Resnet50），这里的Normalize使用的是LayerNorm而非BatchNorm，至于二者的差别，大家可以自己扩展学习下。这里我们注重实现。\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        self.attention_layer = MultiHeadAttention(d_model, d_model, num_heads) \n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward_layer = FeedForward(d_model, d_model, 0.1) \n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # MultiHeadAttention\n",
    "        attention_output = self.attention_layer(x)\n",
    "        # Add + Normalize\n",
    "        attention_output = self.attention_norm(attention_output + x)\n",
    "        encoded = attention_output\n",
    "\n",
    "        # FeedForward\n",
    "        feedforward_output = self.feedforward_layer(x)\n",
    "        # Add + Normalize\n",
    "        feedforward_output = self.feedforward_norm(feedforward_output + x)\n",
    "        \n",
    "        return feedforward_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "至此，我们就实现了Transformer中的Encoder模块，Transformer的定义已经完成了50%。在Embedding组件的基础上，我们又增加了Encoder组件。\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model):\n",
    "        #InputEmbedding\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, d_model)\n",
    "        #OutputEmbedding\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, d_model)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(input_dim, d_model, num_heads, num_layers)\n",
    "```\n",
    "![](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)\n",
    "\n",
    "让我们来基于encoder来跑一个demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([5, 14, 320])\n",
      "cpu\n",
      "cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 5, 14, 160]          51,360\n",
      "            Linear-2           [-1, 5, 14, 160]          51,360\n",
      "            Linear-3           [-1, 5, 14, 160]          51,360\n",
      "     SelfAttention-4  [[-1, 5, 14, 160], [-1, 5, 14, 14]]               0\n",
      "            Linear-5           [-1, 5, 14, 160]          51,360\n",
      "            Linear-6           [-1, 5, 14, 160]          51,360\n",
      "            Linear-7           [-1, 5, 14, 160]          51,360\n",
      "     SelfAttention-8  [[-1, 5, 14, 160], [-1, 5, 14, 14]]               0\n",
      "            Linear-9           [-1, 5, 14, 320]         102,720\n",
      "MultiHeadAttention-10           [-1, 5, 14, 320]               0\n",
      "        LayerNorm-11           [-1, 5, 14, 320]             640\n",
      "           Linear-12           [-1, 5, 14, 320]         102,720\n",
      "          Dropout-13           [-1, 5, 14, 320]               0\n",
      "           Linear-14           [-1, 5, 14, 320]         102,720\n",
      "      FeedForward-15           [-1, 5, 14, 320]               0\n",
      "        LayerNorm-16           [-1, 5, 14, 320]             640\n",
      "================================================================\n",
      "Total params: 617,600\n",
      "Trainable params: 617,600\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 165.60\n",
      "Params size (MB): 2.36\n",
      "Estimated Total Size (MB): 168.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "e = sample_data.src.T.contiguous()\n",
    "print(e.device)\n",
    "sample_embedding = nn.Embedding(7964, 320)\n",
    "e = sample_embedding(e)\n",
    "print(e.shape)\n",
    "# 英文词汇表大小: 7964，我们设置embeding dim为320, 8头注意力\n",
    "sample_encoder = EncoderLayer(7964, 320, 2).cpu()\n",
    "# sample_encoder(e).shape\n",
    "summary(sample_encoder, e.shape, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5、Decoder\n",
    "从下图来看，我们可以看到Decoder的构造和Encoder的差别主要是Encoder-Decoder Attention，但是这张图没有把最下面的Self-Attention模块画准确。这个Self-Attention其实就是我们上文的Mask-Multi-Head-Attention。\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "3.3说完Mask-Multi-Head-Attention，我们继续看Encoder-Decoder Attention。\n",
    "Encoder-Decoder Attention与Self-Attention的最大差别其实就是Attention中的Query来自解码器，而Key和Value来自编码器。上图中encoder和decoder连接的虚线就是代表着这个差别。这样我们又可以进化一下我们的Self-Attention的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6、End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, d_model)\n",
    "        self.key = nn.Linear(input_dim, d_model)\n",
    "        self.value = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "    def forward(self, input_q, input_k, input_v, mask=None):\n",
    "        # 获取输入序列的长度\n",
    "        seq_len = input_q.size(1)\n",
    "        \n",
    "        # 计算query、key、value\n",
    "        query = self.query(input_q)\n",
    "        key = self.key(input_k)\n",
    "        value = self.value(input_v)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores / torch.sqrt(torch.tensor(self.d_model)), dim=-1)\n",
    "        # 使用注意力权重加权得到上下文向量\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        return context, attn_weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"Hidden dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(input_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(num_heads * self.head_dim, input_dim)\n",
    "        \n",
    "    def forward(self, input_q, input_k, input_v, mask=None):\n",
    "        attention_outputs = []\n",
    "        \n",
    "        for attention_head in self.attention_heads:\n",
    "            context, _ = attention_head(input_q, input_k, input_v, mask)\n",
    "            attention_outputs.append(context)\n",
    "        \n",
    "        # 将多个注意力头的输出拼接在一起\n",
    "        concatenated = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # 使用线性层进行投影\n",
    "        output = self.fc(concatenated)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.attention_layer = MultiHeadAttention(d_model, d_model, num_heads) \n",
    "        self.attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward_layer = FeedForward(d_model, d_model, 0.1) \n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        # MultiHeadAttention\n",
    "        attention_output = self.attention_layer(x, x, x)\n",
    "        # Add + Normalize\n",
    "        attention_output = self.attention_norm(attention_output + x)\n",
    "\n",
    "        # FeedForward\n",
    "        feedforward_output = self.feedforward_layer(attention_output)\n",
    "        # Add + Normalize\n",
    "        feedforward_output = self.feedforward_norm(feedforward_output + attention_output)\n",
    "        encoded = feedforward_output\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, d_model, num_heads)\n",
    "        self.encoder_attention = MultiHeadAttention(d_model, d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_model, 0.1)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask):\n",
    "        self_attention_output = self.self_attention(x, x, x, mask)\n",
    "        x = x + self_attention_output\n",
    "        x = self.norm(x)\n",
    "\n",
    "        encoder_attention_output = self.encoder_attention(x, encoder_output, encoder_output, mask)\n",
    "        x = x + encoder_attention_output\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_output, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, d_model, num_heads, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model)\n",
    "        self.encoder = Encoder(d_model, num_heads, num_layers)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_layers)\n",
    "        self.output_linear = nn.Linear(d_model, output_vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq, input_mask, target_mask):\n",
    "        encoder_output = self.encode(input_seq, input_mask)\n",
    "        decoder_output = self.decode(target_seq, encoder_output, target_mask)\n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output\n",
    "\n",
    "    def encode(self, input_seq, input_mask):\n",
    "        embedded_input = self.pos_embedding(self.input_embedding(input_seq))\n",
    "        encoder_output = self.encoder(embedded_input, input_mask)\n",
    "        return encoder_output\n",
    "\n",
    "    def decode(self, target_seq, encoder_output, target_mask):\n",
    "        embedded_target = self.pos_embedding(self.output_embedding(target_seq))\n",
    "        decoder_output = self.decoder(embedded_target, encoder_output, target_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'young,',\n",
       " 'White',\n",
       " 'males',\n",
       " 'are',\n",
       " 'outside',\n",
       " 'near',\n",
       " 'many',\n",
       " 'bushes.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src\n",
    "train_data[0].src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们来定义几个参数\n",
    "\n",
    "'''\n",
    "训练集大小: 29000\n",
    "验证集大小: 1014\n",
    "测试集大小: 1000\n",
    "英文词汇表大小: 7964\n",
    "德文词汇表大小: 9762\n",
    "'''\n",
    "\n",
    "'''\n",
    "在Transformer模型中，d_model是一个超参数，代表模型中隐藏状态的维度或特征的维度。它是指输入、输出和内部表示中的向量维度。\n",
    "\n",
    "在Transformer的编码器和解码器中，所有的嵌入层、注意力机制、前馈神经网络等层的输入和输出向量的维度都是d_model。\n",
    "\n",
    "这个超参数的选择对于Transformer模型的性能和表示能力具有重要影响。较大的d_model可以提供更丰富的特征表示能力，但也会增加模型的计算和存储成本。通常情况下，较常见的d_model取值为128、256、512等。实际中，选择合适的d_model大小需要根据任务的复杂性和数据集的大小来进行调整。\n",
    "\n",
    "需要注意的是，d_model的值必须在整个Transformer模型中保持一致，以确保输入和输出的一致性，并且层之间的连接和参数共享都能够正确地进行。\n",
    "'''\n",
    "\n",
    "input_vocab_size = 7964\n",
    "output_vocab_size = 9762\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "model = Transformer(input_vocab_size, output_vocab_size, d_model, num_heads, num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n",
      "torch.Size([5, 13, 13])\n",
      "输入shape:  torch.Size([5, 14])\n",
      "输出shape:  torch.Size([5, 13])\n",
      "torch.Size([5, 12, 9762])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量\n",
    "    \"\"\"这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)\n",
    "    encoder和decoder都可能调用这个函数，所以seq_len视情况而定\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()  # 这个seq_q只是用来expand维度的\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    # 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]\n",
    "    # [batch_size, 1, len_k], True is masked\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    # [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"建议打印出来看看是什么的输出（一目了然）\n",
    "    seq: [batch_size, tgt_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, tgt_len, tgt_len]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    print(subsequence_mask)\n",
    "    print(subsequence_mask.shape)\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=5,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model.train()\n",
    "for item in train_iterator:\n",
    "    src = item.src.T\n",
    "    trg = item.trg.T\n",
    "    sequence_mask = get_attn_subsequence_mask(trg)\n",
    "    print(\"输入shape: \", src.shape)\n",
    "    print(\"输出shape: \", trg.shape)\n",
    "    x = model(src, trg[:, :-1], None, None)\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
